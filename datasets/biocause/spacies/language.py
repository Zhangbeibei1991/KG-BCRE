import spacy
import numpy
from spacy.tokens import Token
from spacy.tokens import Doc
from spacy.attrs import ORTH, NORM
from spacy.language import Language
from datasets.biocause.spacies.annotation import annotation_gold_entity

def set_token_extension():
    # Set the extensions to keep track of the entities/triggers
    Token.set_extension("is_entity", default=False, force=True)
    Token.set_extension("entity_id", default=None, force=True)
    Token.set_extension("entity_type", default=None, force=True)

    Token.set_extension("is_trigger", default=False, force=True)
    Token.set_extension("trigger_id", default=None, force=True)
    Token.set_extension("trigger_type", default=None, force=True)

    Token.set_extension("arg_type", default=None, force=True)
    Token.set_extension("arg_of_id", default=None, force=True)
    Token.set_extension("arg_of_position", default=None, force=True)
    Token.set_extension("arg_of_ev_type", default=None, force=True)

    Token.set_extension("span", default=None, force=True)

def set_document_extension():
    # Set attribute extensions to the document object
    Doc.set_extension("id", default=None, force=True)
    Doc.set_extension("start_char", default=None, force=True)
    Doc.set_extension("entities", default=None, force=True)
    Doc.set_extension("triggers", default=None, force=True)
    Doc.set_extension("edges", default=None, force=True)

def constrain_tokenizer(nlp):
    nlp.tokenizer.add_special_case("PmrA/PmrB", [{ORTH: "PmrA"}, {ORTH: "/"}, {ORTH: "PmrB"}])
    nlp.tokenizer.add_special_case("sensor/response", [{ORTH: "sensor"}, {ORTH: "/"}, {ORTH: "response"}])
    nlp.tokenizer.add_special_case("bacterial/immunological", [{ORTH: "bacterial"}, {ORTH: "/"}, {ORTH: "immunological"}])
    nlp.tokenizer.add_special_case("recombination/repair", [{ORTH: "recombination"}, {ORTH: "/"}, {ORTH: "repair"}])
    nlp.tokenizer.add_special_case("and/or", [{ORTH: "and"}, {ORTH: "/"}, {ORTH: "or"}])
    nlp.tokenizer.add_special_case("pH/low", [{ORTH: "pH"}, {ORTH: "/"}, {ORTH: "low"}])
    nlp.tokenizer.add_special_case("Ser/Thr", [{ORTH: "Ser"}, {ORTH: "/"}, {ORTH: "Thr"}])
    nlp.tokenizer.add_special_case("LC-MS/MS", [{ORTH: "LC"}, {ORTH: "-"}, {ORTH: "MS"}, {ORTH: "/"}, {ORTH: "MS"}])
    nlp.tokenizer.add_special_case("LC-MS-MS", [{ORTH: "LC"}, {ORTH: "-"}, {ORTH: "MS"}, {ORTH: "-"}, {ORTH: "MS"}])
    nlp.tokenizer.add_special_case("anti-Ser/Thr", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "Ser"}, {ORTH: "/"}, {ORTH: "Thr"}])
    nlp.tokenizer.add_special_case("anti-sigmaE", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "sigmaE"}])
    nlp.tokenizer.add_special_case("anti-phospho", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "phospho"}, ])
    nlp.tokenizer.add_special_case("chaperone-usher", [{ORTH: "chaperone"}, {ORTH: "-"}, {ORTH: "usher"}, ])
    nlp.tokenizer.add_special_case("rapidly-responding", [{ORTH: "rapidly"}, {ORTH: "-"}, {ORTH: "responding"}, ])
    nlp.tokenizer.add_special_case("kinase-response", [{ORTH: "kinase"}, {ORTH: "-"}, {ORTH: "response"}, ])
    nlp.tokenizer.add_special_case("network/circuit", [{ORTH: "network"}, {ORTH: "/"}, {ORTH: "circuit"}])
    nlp.tokenizer.add_special_case("toxin/antitoxin", [{ORTH: "toxin"}, {ORTH: "/"}, {ORTH: "antitoxin"}])
    nlp.tokenizer.add_special_case("approximately3", [{ORTH: "approximately"}, {ORTH: "3"}])
    nlp.tokenizer.add_special_case("approximately34", [{ORTH: "approximately"}, {ORTH: "34"}])
    nlp.tokenizer.add_special_case("approximately100", [{ORTH: "approximately"}, {ORTH: "100"}])
    nlp.tokenizer.add_special_case("al.", [{ORTH: "al"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("Fig.", [{ORTH: "Fig"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("X.", [{ORTH: "X"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("Y.", [{ORTH: "Y"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("P.", [{ORTH: "P"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("S.", [{ORTH: "S"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("E.", [{ORTH: "E"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("A.", [{ORTH: "A"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("C.", [{ORTH: "C"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("M.", [{ORTH: "M"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("B.", [{ORTH: "B"}, {ORTH: "."}])
    nlp.tokenizer.add_special_case("-mediated", [{ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("-protein", [{ORTH: "-"}, {ORTH: "protein"}])
    nlp.tokenizer.add_special_case("-encoded", [{ORTH: "-"}, {ORTH: "encoded"}])
    nlp.tokenizer.add_special_case("-regulated", [{ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("-lac", [{ORTH: "-"}, {ORTH: "lac"}])
    nlp.tokenizer.add_special_case("-binding", [{ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("-activated", [{ORTH: "-"}, {ORTH: "activated"}])
    nlp.tokenizer.add_special_case("-dependent", [{ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("-dependence", [{ORTH: "-"}, {ORTH: "dependence"}])
    nlp.tokenizer.add_special_case("-independent", [{ORTH: "-"}, {ORTH: "independent"}])
    nlp.tokenizer.add_special_case("-hydrolyzing", [{ORTH: "-"}, {ORTH: "hydrolyzing"}])
    nlp.tokenizer.add_special_case("-containing", [{ORTH: "-"}, {ORTH: "containing"}])
    nlp.tokenizer.add_special_case("-HA", [{ORTH: "-"}, {ORTH: "HA"}])
    nlp.tokenizer.add_special_case("-886", [{ORTH: "-"}, {ORTH: "886"}])
    nlp.tokenizer.add_special_case("-mammal", [{ORTH: "-"}, {ORTH: "mammal"}])
    nlp.tokenizer.add_special_case("-response", [{ORTH: "-"}, {ORTH: "response"}])
    nlp.tokenizer.add_special_case("-mediated", [{ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("-like", [{ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("-associated", [{ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("-negative", [{ORTH: "-"}, {ORTH: "negative"}])
    nlp.tokenizer.add_special_case("-expressed", [{ORTH: "-"}, {ORTH: "expressed"}])
    nlp.tokenizer.add_special_case("-mutated", [{ORTH: "-"}, {ORTH: "mutated"}])
    nlp.tokenizer.add_special_case("-ko", [{ORTH: "-"}, {ORTH: "ko"}])
    nlp.tokenizer.add_special_case("acid-", [{ORTH: "acid"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("low-", [{ORTH: "low"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("inter-", [{ORTH: "inter"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("WT-", [{ORTH: "WT"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("SPF-", [{ORTH: "SPF"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("DNA-", [{ORTH: "DNA"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("anti-", [{ORTH: "anti"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("effector-", [{ORTH: "effector"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("4-", [{ORTH: "4"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("co-", [{ORTH: "co"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("beta-", [{ORTH: "beta"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("alpha-", [{ORTH: "alpha"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("insect-", [{ORTH: "insect"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("pan-", [{ORTH: "pan"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("non-", [{ORTH: "non"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("post-", [{ORTH: "post"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("antibiotic-", [{ORTH: "antibiotic"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("phagocytosis-", [{ORTH: "phagocytosis"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("-invasive", [{ORTH: "-"}, {ORTH: "invasive"}])
    nlp.tokenizer.add_special_case("-deficient", [{ORTH: "-"}, {ORTH: "deficient"}])
    nlp.tokenizer.add_special_case("-deletion", [{ORTH: "-"}, {ORTH: "deletion"}])
    nlp.tokenizer.add_special_case("-domain", [{ORTH: "-"}, {ORTH: "domain"}])
    nlp.tokenizer.add_special_case("-based", [{ORTH: "-"}, {ORTH: "based"}])
    nlp.tokenizer.add_special_case("-bound", [{ORTH: "-"}, {ORTH: "bound"}])
    nlp.tokenizer.add_special_case("-resolution", [{ORTH: "-"}, {ORTH: "resolution"}])
    nlp.tokenizer.add_special_case("-specific", [{ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("-Specific", [{ORTH: "-"}, {ORTH: "Specific"}])
    nlp.tokenizer.add_special_case("-FLAG", [{ORTH: "-"}, {ORTH: "FLAG"}])
    nlp.tokenizer.add_special_case("-borne", [{ORTH: "-"}, {ORTH: "borne"}])
    nlp.tokenizer.add_special_case("-scavenging", [{ORTH: "-"}, {ORTH: "scavenging"}])
    nlp.tokenizer.add_special_case("-ortholog", [{ORTH: "-"}, {ORTH: "ortholog"}])
    nlp.tokenizer.add_special_case("-digested", [{ORTH: "-"}, {ORTH: "digested"}])
    nlp.tokenizer.add_special_case("-deficient", [{ORTH: "-"}, {ORTH: "deficient"}])
    nlp.tokenizer.add_special_case("-infected", [{ORTH: "-"}, {ORTH: "infected"}])
    nlp.tokenizer.add_special_case("-replete", [{ORTH: "-"}, {ORTH: "replete"}])
    nlp.tokenizer.add_special_case("-genotype", [{ORTH: "-"}, {ORTH: "genotype"}])
    nlp.tokenizer.add_special_case("downregulated", [{ORTH: "down"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("downregulation", [{ORTH: "down"}, {ORTH: "regulation"}])
    nlp.tokenizer.add_special_case("upregulation", [{ORTH: "up"}, {ORTH: "regulation"}])
    nlp.tokenizer.add_special_case("80degrees", [{ORTH: "80"}, {ORTH: "degrees"}])
    nlp.tokenizer.add_special_case("WT/", [{ORTH: "WT"}, {ORTH: "/"}])
    nlp.tokenizer.add_special_case("phospho-", [{ORTH: "phospho"}, {ORTH: "-"}])
    nlp.tokenizer.add_special_case("Wild-type", [{ORTH: "Wild"}, {ORTH: "-"}, {ORTH: "type"}])
    nlp.tokenizer.add_special_case("wild-type", [{ORTH: "wild"}, {ORTH: "-"}, {ORTH: "type"}])
    nlp.tokenizer.add_special_case("W-Beijing", [{ORTH: "W"}, {ORTH: "-"}, {ORTH: "Beijing"}])
    nlp.tokenizer.add_special_case("tandem-type", [{ORTH: "tandem"}, {ORTH: "-"}, {ORTH: "type"}])
    nlp.tokenizer.add_special_case("long-term", [{ORTH: "long"}, {ORTH: "-"}, {ORTH: "term"}])
    nlp.tokenizer.add_special_case("slow-growing", [{ORTH: "slow"}, {ORTH: "-"}, {ORTH: "growing"}])
    nlp.tokenizer.add_special_case("surface-adhered", [{ORTH: "surface"}, {ORTH: "-"}, {ORTH: "adhered"}])
    nlp.tokenizer.add_special_case("biofilm-associated", [{ORTH: "biofilm"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("DNA-associated", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("DNA-rich", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "rich"}])
    nlp.tokenizer.add_special_case("disease-specific", [{ORTH: "disease"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("animal-passaged", [{ORTH: "animal"}, {ORTH: "-"}, {ORTH: "passaged"}])
    nlp.tokenizer.add_special_case("force-directed", [{ORTH: "force"}, {ORTH: "-"}, {ORTH: "directed"}])
    nlp.tokenizer.add_special_case("PhoP-expressing", [{ORTH: "PhoP"}, {ORTH: "-"}, {ORTH: "expressing"}])
    nlp.tokenizer.add_special_case("innate-immune", [{ORTH: "innate"}, {ORTH: "-"}, {ORTH: "immune"}])
    nlp.tokenizer.add_special_case("bacteria-insect", [{ORTH: "bacteria"}, {ORTH: "-"}, {ORTH: "insect"}])
    nlp.tokenizer.add_special_case("pathogen-insect", [{ORTH: "pathogen"}, {ORTH: "-"}, {ORTH: "insect"}])
    nlp.tokenizer.add_special_case("insect-colonizing", [{ORTH: "insect"}, {ORTH: "-"}, {ORTH: "colonizing"}])
    nlp.tokenizer.add_special_case("invertebrate-specific", [{ORTH: "invertebrate"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("interferon-gamma", [{ORTH: "interferon"}, {ORTH: "-"}, {ORTH: "gamma"}])
    nlp.tokenizer.add_special_case("immuno-deficient", [{ORTH: "immuno"}, {ORTH: "-"}, {ORTH: "deficient"}])
    nlp.tokenizer.add_special_case("real-time", [{ORTH: "real"}, {ORTH: "-"}, {ORTH: "time"}])
    nlp.tokenizer.add_special_case("sensing-like", [{ORTH: "sensing"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("tc-like", [{ORTH: "tc"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("t-statistics", [{ORTH: "t"}, {ORTH: "-"}, {ORTH: "statistics"}])
    nlp.tokenizer.add_special_case("large-scale", [{ORTH: "large"}, {ORTH: "-"}, {ORTH: "scale"}])
    nlp.tokenizer.add_special_case("mammal-pathogen", [{ORTH: "mammal"}, {ORTH: "-"}, {ORTH: "pathogen"}])
    nlp.tokenizer.add_special_case("post-infection", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "infection"}])
    nlp.tokenizer.add_special_case("symptom-free", [{ORTH: "symptom"}, {ORTH: "-"}, {ORTH: "free"}])
    nlp.tokenizer.add_special_case("fibronectin-binding", [{ORTH: "fibronectin"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("RNA-binding", [{ORTH: "RNA"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("adherence-mediated", [{ORTH: "adherence"}, {ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("nutrient-limited", [{ORTH: "nutrient"}, {ORTH: "-"}, {ORTH: "limited"}])
    nlp.tokenizer.add_special_case("saliva-coated", [{ORTH: "saliva"}, {ORTH: "-"}, {ORTH: "coated"}])
    nlp.tokenizer.add_special_case("His-tagged", [{ORTH: "His"}, {ORTH: "-"}, {ORTH: "tagged"}])
    nlp.tokenizer.add_special_case("knock-ins", [{ORTH: "knock"}, {ORTH: "-"}, {ORTH: "ins"}])
    nlp.tokenizer.add_special_case("energy-coupling", [{ORTH: "energy"}, {ORTH: "-"}, {ORTH: "coupling"}])
    nlp.tokenizer.add_special_case("down-regulate", [{ORTH: "down"}, {ORTH: "-"}, {ORTH: "regulate"}])
    nlp.tokenizer.add_special_case("feed-forward", [{ORTH: "feed"}, {ORTH: "-"}, {ORTH: "forward"}])
    nlp.tokenizer.add_special_case("SPI1-encoded", [{ORTH: "SPI1"}, {ORTH: "-"}, {ORTH: "encoded"}])
    nlp.tokenizer.add_special_case("plasmid-encoded", [{ORTH: "plasmid"}, {ORTH: "-"}, {ORTH: "encoded"}])
    nlp.tokenizer.add_special_case("Zn-dependent", [{ORTH: "Zn"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("broad-spectrum", [{ORTH: "broad"}, {ORTH: "-"}, {ORTH: "spectrum"}])
    nlp.tokenizer.add_special_case("post-translational", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "translational"}])
    nlp.tokenizer.add_special_case("in-frame", [{ORTH: "in"}, {ORTH: "-"}, {ORTH: "frame"}])
    nlp.tokenizer.add_special_case("sub-inhibitory", [{ORTH: "sub"}, {ORTH: "-"}, {ORTH: "inhibitory"}])
    nlp.tokenizer.add_special_case("Sub-inhibitory", [{ORTH: "Sub"}, {ORTH: "-"}, {ORTH: "inhibitory"}])
    nlp.tokenizer.add_special_case("sub-lethal", [{ORTH: "sub"}, {ORTH: "-"}, {ORTH: "lethal"}])
    nlp.tokenizer.add_special_case("AND-gate", [{ORTH: "AND"}, {ORTH: "-"}, {ORTH: "gate"}])
    nlp.tokenizer.add_special_case("z-score", [{ORTH: "z"}, {ORTH: "-"}, {ORTH: "score"}])
    nlp.tokenizer.add_special_case("z-scores", [{ORTH: "z"}, {ORTH: "-"}, {ORTH: "scores"}])
    nlp.tokenizer.add_special_case("three-time", [{ORTH: "three"}, {ORTH: "-"}, {ORTH: "time"}])
    nlp.tokenizer.add_special_case("non-immune", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "immune"}])
    nlp.tokenizer.add_special_case("non-invasive", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "invasive"}])
    nlp.tokenizer.add_special_case("non-growing", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "growing"}])
    nlp.tokenizer.add_special_case("chaperone-like", [{ORTH: "chaperone"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("Post-transcriptional", [{ORTH: "Post"}, {ORTH: "-"}, {ORTH: "transcriptional"}])
    nlp.tokenizer.add_special_case("L-glutamate", [{ORTH: "L"}, {ORTH: "-"}, {ORTH: "glutamate"}])
    nlp.tokenizer.add_special_case("protein-bound", [{ORTH: "protein"}, {ORTH: "-"}, {ORTH: "bound"}])
    nlp.tokenizer.add_special_case("tandem-domain", [{ORTH: "tandem"}, {ORTH: "-"}, {ORTH: "domain"}])
    nlp.tokenizer.add_special_case("single-domain", [{ORTH: "single"}, {ORTH: "-"}, {ORTH: "domain"}])
    nlp.tokenizer.add_special_case("nucleotide-free", [{ORTH: "nucleotide"}, {ORTH: "-"}, {ORTH: "free"}])
    nlp.tokenizer.add_special_case("two-week", [{ORTH: "two"}, {ORTH: "-"}, {ORTH: "week"}])
    nlp.tokenizer.add_special_case("three-component", [{ORTH: "three"}, {ORTH: "-"}, {ORTH: "component"}])
    nlp.tokenizer.add_special_case("beta-lactams", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "lactams"}])
    nlp.tokenizer.add_special_case("short-time", [{ORTH: "short"}, {ORTH: "-"}, {ORTH: "time"}])
    nlp.tokenizer.add_special_case("contact-dependent", [{ORTH: "contact"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("beta-hemolysin", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "hemolysin"}])
    nlp.tokenizer.add_special_case("cation-limited", [{ORTH: "cation"}, {ORTH: "-"}, {ORTH: "limited"}])
    nlp.tokenizer.add_special_case("disease-associated", [{ORTH: "disease"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("pore-forming", [{ORTH: "pore"}, {ORTH: "-"}, {ORTH: "forming"}])
    nlp.tokenizer.add_special_case("genetically-manipulated", [{ORTH: "genetically"}, {ORTH: "-"}, {ORTH: "manipulated"}])
    nlp.tokenizer.add_special_case("pili-dependent", [{ORTH: "pili"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("96-well", [{ORTH: "96"}, {ORTH: "-"}, {ORTH: "well"}])
    nlp.tokenizer.add_special_case("4degreesC", [{ORTH: "4"}, {ORTH: "degrees"}, {ORTH: "C"}])
    nlp.tokenizer.add_special_case("-70degreesC", [{ORTH: "-70"}, {ORTH: "degrees"}, {ORTH: "C"}])
    nlp.tokenizer.add_special_case("growth-regulatory", [{ORTH: "growth"}, {ORTH: "-"}, {ORTH: "regulatory"}])
    nlp.tokenizer.add_special_case("growth-regulating", [{ORTH: "growth"}, {ORTH: "-"}, {ORTH: "regulating"}])
    nlp.tokenizer.add_special_case("alpha-crystallin", [{ORTH: "alpha"}, {ORTH: "-"}, {ORTH: "crystallin"}])
    nlp.tokenizer.add_special_case("alpha-ketoglutarate", [{ORTH: "alpha"}, {ORTH: "-"}, {ORTH: "ketoglutarate"}])
    nlp.tokenizer.add_special_case("alpha-helices", [{ORTH: "alpha"}, {ORTH: "-"}, {ORTH: "helices"}])
    nlp.tokenizer.add_special_case("pre-screening", [{ORTH: "pre"}, {ORTH: "-"}, {ORTH: "screening"}])
    nlp.tokenizer.add_special_case("nucleotide-binding", [{ORTH: "nucleotide"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("post-conjugation", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "conjugation"}])
    nlp.tokenizer.add_special_case("warm-blooded", [{ORTH: "warm"}, {ORTH: "-"}, {ORTH: "blooded"}])
    nlp.tokenizer.add_special_case("DNA-induced", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "induced"}])
    nlp.tokenizer.add_special_case("DNA-enriched", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "enriched"}])
    nlp.tokenizer.add_special_case("DNA-dependent", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("pre-loaded", [{ORTH: "pre"}, {ORTH: "-"}, {ORTH: "loaded"}])
    nlp.tokenizer.add_special_case("beta-lactam", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "lactam"}])
    nlp.tokenizer.add_special_case("planktonically-grown", [{ORTH: "planktonically"}, {ORTH: "-"}, {ORTH: "grown"}])
    nlp.tokenizer.add_special_case("Dispersion-stage", [{ORTH: "Dispersion"}, {ORTH: "-"}, {ORTH: "stage"}])
    nlp.tokenizer.add_special_case("SPI1-inducing", [{ORTH: "SPI1"}, {ORTH: "-"}, {ORTH: "inducing"}])
    nlp.tokenizer.add_special_case("100-fold", [{ORTH: "100"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("2-fold", [{ORTH: "2"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("2.5-fold", [{ORTH: "2.5"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("3-fold", [{ORTH: "3"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("4-fold", [{ORTH: "4"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("log2-fold", [{ORTH: "log2"}, {ORTH: "-"}, {ORTH: "fold"}])
    # nlp.tokenizer.add_special_case("20-fold", [{ORTH: "4"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("16-fold", [{ORTH: "16"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("2560-fold", [{ORTH: "2560"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("64-fold", [{ORTH: "64"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("17-amino", [{ORTH: "17"}, {ORTH: "-"}, {ORTH: "amino"}])
    nlp.tokenizer.add_special_case("in-frame", [{ORTH: "in"}, {ORTH: "-"}, {ORTH: "frame"}])
    nlp.tokenizer.add_special_case("co-evolved", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "evolved"}])
    nlp.tokenizer.add_special_case("co-culture", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "culture"}])
    nlp.tokenizer.add_special_case("co-factor", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "factor"}])
    nlp.tokenizer.add_special_case("co-evolution", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "evolution"}])
    nlp.tokenizer.add_special_case("co-regulated", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("co-expressed", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "expressed"}])
    nlp.tokenizer.add_special_case("co-expression", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "expression"}])
    nlp.tokenizer.add_special_case("co-transcriptional", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "transcriptional"}])
    nlp.tokenizer.add_special_case("systems-level", [{ORTH: "systems"}, {ORTH: "-"}, {ORTH: "level"}])
    nlp.tokenizer.add_special_case("Walker-A", [{ORTH: "Walker"}, {ORTH: "-"}, {ORTH: "A"}])
    nlp.tokenizer.add_special_case("Walker-B", [{ORTH: "Walker"}, {ORTH: "-"}, {ORTH: "B"}])
    nlp.tokenizer.add_special_case("lysyl-phosphatidylglycerol", [{ORTH: "lysyl"}, {ORTH: "-"}, {ORTH: "phosphatidylglycerol"}])
    nlp.tokenizer.add_special_case("serotype-determinant", [{ORTH: "serotype"}, {ORTH: "-"}, {ORTH: "determinant"}])
    nlp.tokenizer.add_special_case("plasmid-complemented", [{ORTH: "plasmid"}, {ORTH: "-"}, {ORTH: "complemented"}])
    nlp.tokenizer.add_special_case("D-alanine", [{ORTH: "D"}, {ORTH: "-"}, {ORTH: "alanine"}])
    nlp.tokenizer.add_special_case("co-purifies", [{ORTH: "co"}, {ORTH: "-"}, {ORTH: "purifies"}])
    nlp.tokenizer.add_special_case("de-repressor", [{ORTH: "de"}, {ORTH: "-"}, {ORTH: "repressor"}])
    nlp.tokenizer.add_special_case("trans-acting", [{ORTH: "trans"}, {ORTH: "-"}, {ORTH: "acting"}])
    nlp.tokenizer.add_special_case("life-threatening", [{ORTH: "life"}, {ORTH: "-"}, {ORTH: "threatening"}])
    nlp.tokenizer.add_special_case("invasion-associated", [{ORTH: "invasion"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("high-level", [{ORTH: "high"}, {ORTH: "-"}, {ORTH: "level"}])
    nlp.tokenizer.add_special_case("protein-DNA", [{ORTH: "protein"}, {ORTH: "-"}, {ORTH: "DNA"}])
    nlp.tokenizer.add_special_case("regulon-inducing", [{ORTH: "regulon"}, {ORTH: "-"}, {ORTH: "inducing"}])
    nlp.tokenizer.add_special_case("concentration-dependent", [{ORTH: "concentration"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("integration-proficient", [{ORTH: "integration"}, {ORTH: "-"}, {ORTH: "proficient"}])
    nlp.tokenizer.add_special_case("glutamate-aspartate", [{ORTH: "glutamate"}, {ORTH: "-"}, {ORTH: "aspartate"}])
    nlp.tokenizer.add_special_case("chemo-attractant", [{ORTH: "chemo"}, {ORTH: "-"}, {ORTH: "attractant"}])
    nlp.tokenizer.add_special_case("host-pathogen", [{ORTH: "host"}, {ORTH: "-"}, {ORTH: "pathogen"}])
    nlp.tokenizer.add_special_case("liquid-based", [{ORTH: "liquid"}, {ORTH: "-"}, {ORTH: "based"}])
    nlp.tokenizer.add_special_case("medium-based", [{ORTH: "medium"}, {ORTH: "-"}, {ORTH: "based"}])
    nlp.tokenizer.add_special_case("semi-automated", [{ORTH: "semi"}, {ORTH: "-"}, {ORTH: "automated"}])
    nlp.tokenizer.add_special_case("dormancy-signaling", [{ORTH: "dormancy"}, {ORTH: "-"}, {ORTH: "signaling"}])
    nlp.tokenizer.add_special_case("Semi-Automated", [{ORTH: "Semi"}, {ORTH: "-"}, {ORTH: "Automated"}])
    nlp.tokenizer.add_special_case("high-throughput", [{ORTH: "high"}, {ORTH: "-"}, {ORTH: "throughput"}])
    nlp.tokenizer.add_special_case("fine-tuning", [{ORTH: "fine"}, {ORTH: "-"}, {ORTH: "tuning"}])
    nlp.tokenizer.add_special_case("protein-tagged", [{ORTH: "protein"}, {ORTH: "-"}, {ORTH: "tagged"}])
    nlp.tokenizer.add_special_case("anti-FLAG", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "FLAG"}])
    nlp.tokenizer.add_special_case("anti-parallel", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "parallel"}])
    nlp.tokenizer.add_special_case("anti-sigma", [{ORTH: "anti"}, {ORTH: "-"}, {ORTH: "sigma"}])
    nlp.tokenizer.add_special_case("eight-gene", [{ORTH: "eight"}, {ORTH: "-"}, {ORTH: "gene"}])
    nlp.tokenizer.add_special_case("nine-gene", [{ORTH: "nine"}, {ORTH: "-"}, {ORTH: "gene"}])
    nlp.tokenizer.add_special_case("single-gene", [{ORTH: "single"}, {ORTH: "-"}, {ORTH: "gene"}])
    nlp.tokenizer.add_special_case("pre-growth", [{ORTH: "pre"}, {ORTH: "-"}, {ORTH: "growth"}])
    nlp.tokenizer.add_special_case("pre-adherence", [{ORTH: "pre"}, {ORTH: "-"}, {ORTH: "adherence"}])
    nlp.tokenizer.add_special_case("collision-induced", [{ORTH: "collision"}, {ORTH: "-"}, {ORTH: "induced"}])
    nlp.tokenizer.add_special_case("user-friendly", [{ORTH: "user"}, {ORTH: "-"}, {ORTH: "friendly"}])
    nlp.tokenizer.add_special_case("exotoxin-encoding", [{ORTH: "exotoxin"}, {ORTH: "-"}, {ORTH: "encoding"}])
    nlp.tokenizer.add_special_case("sortase-encoding", [{ORTH: "sortase"}, {ORTH: "-"}, {ORTH: "encoding"}])
    nlp.tokenizer.add_special_case("fibronectin-mediated", [{ORTH: "fibronectin"}, {ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("surface-exposed", [{ORTH: "surface"}, {ORTH: "-"}, {ORTH: "exposed"}])
    nlp.tokenizer.add_special_case("lambda-proteobacteria", [{ORTH: "lambda"}, {ORTH: "-"}, {ORTH: "proteobacteria"}])
    nlp.tokenizer.add_special_case("gamma-proteobacteria", [{ORTH: "gamma"}, {ORTH: "-"}, {ORTH: "proteobacteria"}])
    nlp.tokenizer.add_special_case("beta-sheet", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "sheet"}])
    nlp.tokenizer.add_special_case("interface-stabilizing", [{ORTH: "interface"}, {ORTH: "-"}, {ORTH: "stabilizing"}])
    nlp.tokenizer.add_special_case("cis-regulatory", [{ORTH: "cis"}, {ORTH: "-"}, {ORTH: "regulatory"}])
    nlp.tokenizer.add_special_case("maturation-1", [{ORTH: "maturation"}, {ORTH: "-"}, {ORTH: "1"}])
    nlp.tokenizer.add_special_case("arginine-specific", [{ORTH: "arginine"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("biofilm-specific", [{ORTH: "biofilm"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("atom-substituted", [{ORTH: "atom"}, {ORTH: "-"}, {ORTH: "substituted"}])
    nlp.tokenizer.add_special_case("site-directed", [{ORTH: "site"}, {ORTH: "-"}, {ORTH: "directed"}])
    nlp.tokenizer.add_special_case("cell-cell", [{ORTH: "cell"}, {ORTH: "-"}, {ORTH: "cell"}])
    nlp.tokenizer.add_special_case("biotin-labeled", [{ORTH: "biotin"}, {ORTH: "-"}, {ORTH: "labeled"}])
    nlp.tokenizer.add_special_case("arabinose-inducible", [{ORTH: "arabinose"}, {ORTH: "-"}, {ORTH: "inducible"}])
    nlp.tokenizer.add_special_case("re-sequencing", [{ORTH: "re"}, {ORTH: "-"}, {ORTH: "sequencing"}])
    nlp.tokenizer.add_special_case("microarray-based", [{ORTH: "microarray"}, {ORTH: "-"}, {ORTH: "based"}])
    nlp.tokenizer.add_special_case("laboratory-attenuated", [{ORTH: "laboratory"}, {ORTH: "-"}, {ORTH: "attenuated"}])
    nlp.tokenizer.add_special_case("well-documented", [{ORTH: "well"}, {ORTH: "-"}, {ORTH: "documented"}])
    nlp.tokenizer.add_special_case("well-established", [{ORTH: "well"}, {ORTH: "-"}, {ORTH: "established"}])
    nlp.tokenizer.add_special_case("well-characterized", [{ORTH: "well"}, {ORTH: "-"}, {ORTH: "characterized"}])
    nlp.tokenizer.add_special_case("well-replicated", [{ORTH: "well"}, {ORTH: "-"}, {ORTH: "replicated"}])
    nlp.tokenizer.add_special_case("reverse-transcriptional", [{ORTH: "reverse"}, {ORTH: "-"}, {ORTH: "transcriptional"}])
    nlp.tokenizer.add_special_case("micro-array", [{ORTH: "micro"}, {ORTH: "-"}, {ORTH: "array"}])
    nlp.tokenizer.add_special_case("knock-in", [{ORTH: "knock"}, {ORTH: "-"}, {ORTH: "in"}])
    nlp.tokenizer.add_special_case("colony-forming", [{ORTH: "colony"}, {ORTH: "-"}, {ORTH: "forming"}])
    nlp.tokenizer.add_special_case("marrow-derived", [{ORTH: "marrow"}, {ORTH: "-"}, {ORTH: "derived"}])
    nlp.tokenizer.add_special_case("non-non", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "non"}])
    nlp.tokenizer.add_special_case("non-adhered", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "adhered"}])
    nlp.tokenizer.add_special_case("peg-adhered", [{ORTH: "peg"}, {ORTH: "-"}, {ORTH: "adhered"}])
    nlp.tokenizer.add_special_case("PMN-Mediated", [{ORTH: "PMN"}, {ORTH: "-"}, {ORTH: "Mediated"}])
    nlp.tokenizer.add_special_case("insect-insect", [{ORTH: "insect"}, {ORTH: "-"}, {ORTH: "insect"}])
    nlp.tokenizer.add_special_case("myo-inositol", [{ORTH: "myo"}, {ORTH: "-"}, {ORTH: "inositol"}])
    # nlp.tokenizer.add_special_case("WT-mediated", [{ORTH: "WT"}, {ORTH: "-"}, {ORTH: "Mediated"}])
    nlp.tokenizer.add_special_case("prophage-mediated", [{ORTH: "prophage"}, {ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("high-affinity", [{ORTH: "high"}, {ORTH: "-"}, {ORTH: "affinity"}])
    nlp.tokenizer.add_special_case("blood-borne", [{ORTH: "blood"}, {ORTH: "-"}, {ORTH: "borne"}])
    nlp.tokenizer.add_special_case("insect-associated", [{ORTH: "insect"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("surface-associated", [{ORTH: "surface"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("insect-specific", [{ORTH: "insect"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("stage-specific", [{ORTH: "stage"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("15degreesC", [{ORTH: "15"}, {ORTH: "degrees"}, {ORTH: "C"}])
    nlp.tokenizer.add_special_case("bacteria-host", [{ORTH: "bacteria"}, {ORTH: "-"}, {ORTH: "host"}])
    nlp.tokenizer.add_special_case("nematode-associated", [{ORTH: "nematode"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("40-kb", [{ORTH: "40"}, {ORTH: "-"}, {ORTH: "kb"}])
    nlp.tokenizer.add_special_case("cross-over", [{ORTH: "cross"}, {ORTH: "-"}, {ORTH: "over"}])
    nlp.tokenizer.add_special_case("domain-containing", [{ORTH: "domain"}, {ORTH: "-"}, {ORTH: "containing"}])
    nlp.tokenizer.add_special_case("hemopexin-like", [{ORTH: "hemopexin"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("terminator-like", [{ORTH: "terminator"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("mesh-like", [{ORTH: "mesh"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("thread-like", [{ORTH: "thread"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("defensin-like", [{ORTH: "defensin"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("bacteria-invertebrate", [{ORTH: "bacteria"}, {ORTH: "-"}, {ORTH: "invertebrate"}])
    # nlp.tokenizer.add_special_case("RT-PCR", [{ORTH: "RT"}, {ORTH: "-"}, {ORTH: "PCR"}])
    nlp.tokenizer.add_special_case("down-regulated", [{ORTH: "down"}, {ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("genome-wide", [{ORTH: "genome"}, {ORTH: "-"}, {ORTH: "wide"}])
    nlp.tokenizer.add_special_case("up-regulated", [{ORTH: "up"}, {ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("over-expression", [{ORTH: "over"}, {ORTH: "-"}, {ORTH: "expression"}])
    nlp.tokenizer.add_special_case("space-filling", [{ORTH: "space"}, {ORTH: "-"}, {ORTH: "filling"}])
    nlp.tokenizer.add_special_case("N-minimal", [{ORTH: "N"}, {ORTH: "-"}, {ORTH: "minimal"}])
    nlp.tokenizer.add_special_case("N-terminal", [{ORTH: "N"}, {ORTH: "-"}, {ORTH: "terminal"}])
    nlp.tokenizer.add_special_case("N-terminus", [{ORTH: "N"}, {ORTH: "-"}, {ORTH: "terminus"}])
    nlp.tokenizer.add_special_case("DNA-binding", [{ORTH: "DNA"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("G-protein", [{ORTH: "G"}, {ORTH: "-"}, {ORTH: "protein"}])
    nlp.tokenizer.add_special_case("low-copy", [{ORTH: "low"}, {ORTH: "-"}, {ORTH: "copy"}])
    nlp.tokenizer.add_special_case("ABC-transporters", [{ORTH: "ABC"}, {ORTH: "-"}, {ORTH: "transporters"}])
    nlp.tokenizer.add_special_case("ABC-type", [{ORTH: "ABC"}, {ORTH: "-"}, {ORTH: "type"}])
    nlp.tokenizer.add_special_case("F0F1-type", [{ORTH: "F0F1"}, {ORTH: "-"}, {ORTH: "type"}])
    nlp.tokenizer.add_special_case("Gram-negative", [{ORTH: "Gram"}, {ORTH: "-"}, {ORTH: "negative"}])
    nlp.tokenizer.add_special_case("gram-negative", [{ORTH: "gram"}, {ORTH: "-"}, {ORTH: "negative"}])
    nlp.tokenizer.add_special_case("PmrA-regulated", [{ORTH: "PmrA"}, {ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("O-antigen", [{ORTH: "O"}, {ORTH: "-"}, {ORTH: "antigen"}])
    nlp.tokenizer.add_special_case("O-antigen", [{ORTH: "O"}, {ORTH: "-"}, {ORTH: "antigen"}])
    nlp.tokenizer.add_special_case("pathogenesis-related", [{ORTH: "pathogenesis"}, {ORTH: "-"}, {ORTH: "related"}])
    nlp.tokenizer.add_special_case("cuticle-covered", [{ORTH: "cuticle"}, {ORTH: "-"}, {ORTH: "covered"}])
    nlp.tokenizer.add_special_case("PmrA-activated", [{ORTH: "PmrA"}, {ORTH: "-"}, {ORTH: "activated"}])
    nlp.tokenizer.add_special_case("two-component", [{ORTH: "two"}, {ORTH: "-"}, {ORTH: "component"}])
    nlp.tokenizer.add_special_case("Two-component", [{ORTH: "Two"}, {ORTH: "-"}, {ORTH: "component"}])
    nlp.tokenizer.add_special_case("wall-anchored", [{ORTH: "wall"}, {ORTH: "-"}, {ORTH: "anchored"}])
    nlp.tokenizer.add_special_case("membrane-anchored", [{ORTH: "membrane"}, {ORTH: "-"}, {ORTH: "anchored"}])
    nlp.tokenizer.add_special_case("Phage-derived", [{ORTH: "Phage"}, {ORTH: "-"}, {ORTH: "derived"}])
    nlp.tokenizer.add_special_case("fast-growing", [{ORTH: "fast"}, {ORTH: "-"}, {ORTH: "growing"}])
    nlp.tokenizer.add_special_case("lipid-modified", [{ORTH: "lipid"}, {ORTH: "-"}, {ORTH: "modified"}])
    nlp.tokenizer.add_special_case("little-reported", [{ORTH: "little"}, {ORTH: "-"}, {ORTH: "reported"}])
    nlp.tokenizer.add_special_case("mammalian-specific", [{ORTH: "mammalian"}, {ORTH: "-"}, {ORTH: "specific"}])
    nlp.tokenizer.add_special_case("shock-like", [{ORTH: "shock"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("Bt-like", [{ORTH: "Bt"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("tcc-like", [{ORTH: "tcc"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("tc-PAIYe", [{ORTH: "tc"}, {ORTH: "-"}, {ORTH: "PAIYe"}])
    nlp.tokenizer.add_special_case("Tcd-like", [{ORTH: "Tcd"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("Tc-like", [{ORTH: "Tc"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("insecticidal-like", [{ORTH: "insecticidal"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case(")-like", [{ORTH: ")"}, {ORTH: "-"}, {ORTH: "like"}])
    # nlp.tokenizer.add_special_case("LysR-type", [{ORTH: "LysR"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("low-temperature", [{ORTH: "low"}, {ORTH: "-"}, {ORTH: "temperature"}])
    nlp.tokenizer.add_special_case("Gram-positive", [{ORTH: "Gram"}, {ORTH: "-"}, {ORTH: "positive"}])
    nlp.tokenizer.add_special_case("type-VI", [{ORTH: "type"}, {ORTH: "-"}, {ORTH: "VI"}])
    nlp.tokenizer.add_special_case("type-2", [{ORTH: "type"}, {ORTH: "-"}, {ORTH: "2"}])
    nlp.tokenizer.add_special_case("type-1", [{ORTH: "type"}, {ORTH: "-"}, {ORTH: "1"}])
    nlp.tokenizer.add_special_case("ten-stranded", [{ORTH: "ten"}, {ORTH: "-"}, {ORTH: "stranded"}])
    nlp.tokenizer.add_special_case("five-stranded", [{ORTH: "five"}, {ORTH: "-"}, {ORTH: "stranded"}])
    nlp.tokenizer.add_special_case("HPLC-based", [{ORTH: "HPLC"}, {ORTH: "-"}, {ORTH: "based"}])
    nlp.tokenizer.add_special_case("mushroom-shaped", [{ORTH: "mushroom"}, {ORTH: "-"}, {ORTH: "shaped"}])
    nlp.tokenizer.add_special_case("cross-strand", [{ORTH: "cross"}, {ORTH: "-"}, {ORTH: "strand"}])
    nlp.tokenizer.add_special_case("A-family", [{ORTH: "A"}, {ORTH: "-"}, {ORTH: "family"}])
    nlp.tokenizer.add_special_case("RTX-family", [{ORTH: "RTX"}, {ORTH: "-"}, {ORTH: "family"}])
    nlp.tokenizer.add_special_case("type-III", [{ORTH: "type"}, {ORTH: "-"}, {ORTH: "III"}])
    nlp.tokenizer.add_special_case("III-associated", [{ORTH: "III"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("T3SS-associated", [{ORTH: "T3SS"}, {ORTH: "-"}, {ORTH: "associated"}])
    nlp.tokenizer.add_special_case("multi-cargo", [{ORTH: "multi"}, {ORTH: "-"}, {ORTH: "cargo"}])
    nlp.tokenizer.add_special_case("multi-component", [{ORTH: "multi"}, {ORTH: "-"}, {ORTH: "component"}])
    nlp.tokenizer.add_special_case("Sec-system", [{ORTH: "Sec"}, {ORTH: "-"}, {ORTH: "system"}])
    nlp.tokenizer.add_special_case("multi-copy", [{ORTH: "multi"}, {ORTH: "-"}, {ORTH: "copy"}])
    nlp.tokenizer.add_special_case("low-level", [{ORTH: "low"}, {ORTH: "-"}, {ORTH: "level"}])
    nlp.tokenizer.add_special_case("effector-chaperone", [{ORTH: "effector"}, {ORTH: "-"}, {ORTH: "chaperone"}])
    nlp.tokenizer.add_special_case("temperature-matched", [{ORTH: "temperature"}, {ORTH: "-"}, {ORTH: "matched"}])
    nlp.tokenizer.add_special_case("chaperone-effector", [{ORTH: "chaperone"}, {ORTH: "-"}, {ORTH: "effector"}])
    nlp.tokenizer.add_special_case("Structure-guided", [{ORTH: "Structure"}, {ORTH: "-"}, {ORTH: "guided"}])
    nlp.tokenizer.add_special_case("chaperone-effector", [{ORTH: "chaperone"}, {ORTH: "-"}, {ORTH: "effector"}])
    nlp.tokenizer.add_special_case("maturation-2", [{ORTH: "maturation"}, {ORTH: "-"}, {ORTH: "2"}])
    nlp.tokenizer.add_special_case("thus-far-unknown",
                                   [{ORTH: "thus"}, {ORTH: "-"}, {ORTH: "far"}, {ORTH: "-"}, {ORTH: "unknown"}])
    nlp.tokenizer.add_special_case("thus-far-unknown",
                                   [{ORTH: "thus"}, {ORTH: "-"}, {ORTH: "far"}, {ORTH: "-"}, {ORTH: "unknown"}])
    nlp.tokenizer.add_special_case("ChIP-on-chip",
                                   [{ORTH: "ChIP"}, {ORTH: "-"}, {ORTH: "on"}, {ORTH: "-"}, {ORTH: "chip"}])
    nlp.tokenizer.add_special_case("SPI-2-encoded",
                                   [{ORTH: "SPI-2"}, {ORTH: "-"}, {ORTH: "encoded"}])
    # nlp.tokenizer.add_special_case("c-di-GMP", [{ORTH: "c"}, {ORTH: "-"}, {ORTH: "di"}, {ORTH: "-"}, {ORTH: "GMP"}])
    nlp.tokenizer.add_special_case("orotidine-5-phosphate",
                                   [{ORTH: "orotidine"}, {ORTH: "-"}, {ORTH: "5"}, {ORTH: "-"}, {ORTH: "phosphate"}])
    nlp.tokenizer.add_special_case("approximately6-fold",
                                   [{ORTH: "approximately"}, {ORTH: "6"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("approximately10-fold",
                                   [{ORTH: "approximately"}, {ORTH: "10"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("approximately20-fold",
                                   [{ORTH: "approximately"}, {ORTH: "20"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("(PMN)-mediated",
                                   [{ORTH: "("}, {ORTH: "PMN"}, {ORTH: ")"}, {ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("(pMT1)-encoded",
                                   [{ORTH: "("}, {ORTH: "pMT1"}, {ORTH: ")"}, {ORTH: "-"}, {ORTH: "encoded"}])
    nlp.tokenizer.add_special_case("mode(s)",
                                   [{ORTH: "mode"}, {ORTH: "("}, {ORTH: "s"}, {ORTH: ")"}])
    nlp.tokenizer.add_special_case("signal(s)",
                                   [{ORTH: "signal"}, {ORTH: "("}, {ORTH: "s"}, {ORTH: ")"}])
    nlp.tokenizer.add_special_case("(in)directly",
                                   [{ORTH: "("}, {ORTH: "in"}, {ORTH: ")"}, {ORTH: "directly"}])
    nlp.tokenizer.add_special_case("(de)phosphorylation",
                                   [{ORTH: "("}, {ORTH: "de"}, {ORTH: ")"}, {ORTH: "phosphorylation"}])
    nlp.tokenizer.add_special_case("hemolysin/hemagglutinin-related",
                                   [{ORTH: "hemolysin"}, {ORTH: "/"}, {ORTH: "hemagglutinin"}, {ORTH: "-"},
                                    {ORTH: "related"}])
    # nlp.tokenizer.add_special_case("toxin-(dnt-	1564-1575)", [{ORTH: "toxin"}, {ORTH: "-"}, {ORTH: "dnt"}, {ORTH: "-"},
    #                                                              {ORTH: "1564"},{ORTH: "-"},{ORTH: "1575"}, {ORTH: ")"}])
    nlp.tokenizer.add_special_case("adenine/ribose-binding",
                                   [{ORTH: "adenine"}, {ORTH: "/"}, {ORTH: "ribose"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("phosphoryl/ribosyl-binding",
                                   [{ORTH: "phosphoryl"}, {ORTH: "/"}, {ORTH: "ribosyl"}, {ORTH: "-"}, {ORTH: "binding"}])
    nlp.tokenizer.add_special_case("colicin/pyocin-like",
                                   [{ORTH: "colicin"}, {ORTH: "/"}, {ORTH: "pyocin"}, {ORTH: "-"}, {ORTH: "like"}])
    nlp.tokenizer.add_special_case("helix-turn-helix",
                                   [{ORTH: "helix"}, {ORTH: "-"}, {ORTH: "turn"}, {ORTH: "-"}, {ORTH: "helix"}])
    nlp.tokenizer.add_special_case("[12]-[14]", [{ORTH: "["}, {ORTH: "12"}, {ORTH: "]"}, {ORTH: "-"}, {ORTH: "["}, {ORTH: "14"}, {ORTH: "]"}])
    nlp.tokenizer.add_special_case("[44],[45]", [{ORTH: "["}, {ORTH: "44"}, {ORTH: "]"}, {ORTH: ","}, {ORTH: "["}, {ORTH: "45"}, {ORTH: "]"}])
    nlp.tokenizer.add_special_case("[41],[42],[43]", [{ORTH: "["}, {ORTH: "41"}, {ORTH: "]"}, {ORTH: ","},
                                                      {ORTH: "["}, {ORTH: "42"}, {ORTH: "]"},{ORTH: ","}, {ORTH: "["}, {ORTH: "43"}, {ORTH: "]"}])
    nlp.tokenizer.add_special_case("2A-C", [{ORTH: "2A"}, {ORTH: "-"}, {ORTH: "C"}])
    nlp.tokenizer.add_special_case("Bis-Tris", [{ORTH: "Bis"}, {ORTH: "-"}, {ORTH: "Tris"}])
    nlp.tokenizer.add_special_case("pH-dependent", [{ORTH: "pH"}, {ORTH: "-"}, {ORTH: "dependent"}])
    nlp.tokenizer.add_special_case("transcription-PCR", [{ORTH: "transcription"}, {ORTH: "-"}, {ORTH: "PCR"}])
    nlp.tokenizer.add_special_case("beta-galactosidase", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "galactosidase"}])
    nlp.tokenizer.add_special_case("beta-strands", [{ORTH: "beta"}, {ORTH: "-"}, {ORTH: "strands"}])
    nlp.tokenizer.add_special_case("non-protein", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "protein"}])
    nlp.tokenizer.add_special_case("non-flagellar", [{ORTH: "non"}, {ORTH: "-"}, {ORTH: "flagellar"}])
    nlp.tokenizer.add_special_case("whole-genome", [{ORTH: "whole"}, {ORTH: "-"}, {ORTH: "genome"}])
    nlp.tokenizer.add_special_case("chaperone-ATPase", [{ORTH: "chaperone"}, {ORTH: "-"}, {ORTH: "ATPase"}])
    nlp.tokenizer.add_special_case("SsrB-regulated", [{ORTH: "SsrB"}, {ORTH: "-"}, {ORTH: "regulated"}])
    nlp.tokenizer.add_special_case("Ser-kinase", [{ORTH: "Ser"}, {ORTH: "-"}, {ORTH: "kinase"}])
    nlp.tokenizer.add_special_case("post-transmission", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "transmission"}])
    nlp.tokenizer.add_special_case("post-infection", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "infection"}])
    nlp.tokenizer.add_special_case("post-transcriptional", [{ORTH: "post"}, {ORTH: "-"}, {ORTH: "transcriptional"}])
    nlp.tokenizer.add_special_case("28degreesC", [{ORTH: "28"}, {ORTH: "degrees"}, {ORTH: "C"}])
    nlp.tokenizer.add_special_case("Two-Component", [{ORTH: "Two"}, {ORTH: "-"}, {ORTH: "Component"}])
    nlp.tokenizer.add_special_case("10-fold", [{ORTH: "10"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("8-fold", [{ORTH: "8"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("1000-fold", [{ORTH: "1000"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("256-fold", [{ORTH: "256"}, {ORTH: "-"}, {ORTH: "fold"}])
    nlp.tokenizer.add_special_case("t-test", [{ORTH: "t"}, {ORTH: "-"}, {ORTH: "test"}])
    nlp.tokenizer.add_special_case("membrane-bound", [{ORTH: "membrane"}, {ORTH: "-"}, {ORTH: "bound"}])
    nlp.tokenizer.add_special_case("PMN-mediated", [{ORTH: "PMN"}, {ORTH: "-"}, {ORTH: "mediated"}])
    nlp.tokenizer.add_special_case("113-member", [{ORTH: "113"}, {ORTH: "-"}, {ORTH: "member"}])
    nlp.tokenizer.add_special_case("forty-eight", [{ORTH: "forty"}, {ORTH: "-"}, {ORTH: "eight"}])
    nlp.tokenizer.add_special_case("forty-one", [{ORTH: "forty"}, {ORTH: "-"}, {ORTH: "one"}])
    nlp.tokenizer.add_special_case("pathway(s)", [{ORTH: "pathway"}, {ORTH: "("}, {ORTH: "s"}, {ORTH: ")"}])
    nlp.tokenizer.add_special_case("molecule(s)", [{ORTH: "molecule"}, {ORTH: "("}, {ORTH: "s"}, {ORTH: ")"}])
    nlp.tokenizer.add_special_case("-binding-like", [{ORTH: "-"}, {ORTH: "binding"}, {ORTH: "-"}, {ORTH: "like"}])


def set_nlp_environment(keep_ent_tokens):
    """A function that sets the NLP workflow (with custom components) along with
    the language model."""
    @Language.component(name='merger')
    def merger(doc):
        indices = []
        indices_left_line = []
        temp = []
        for token in doc:
            temp.append(token.text)

        for token in doc:
            if (token.i > 0):
                if (token.text == "/-") and (doc[token.i - 1].text == "+"):
                    indices.append(token.i)
                if token.text == '-binding-like':
                    print()


        offset = 0
        for index in indices:
            with doc.retokenize() as retokenizer:
                retokenizer.merge(doc[index - 1 - offset: index + 1 - offset])
                offset += 1
        return doc

    @Language.component(name='custom_sentencizer')
    def custom_sentencizer(doc):
        for i, token in enumerate(doc[:-2]):
            if (token.text == "B.") and (doc[i - 1].text.endswith("kappa")) and (doc[i + 1].text[
                0].istitle()):  # more robust than: (doc[i+1].is_title) for cases such as "Transient-transfection"
                doc[i + 1].is_sent_start = True
            elif ((token.text == "A.") or (token.text == "B.") or (token.text == "C.")) and (
                    doc[i - 1].text.endswith("ase") or doc[i - 1].text.endswith("lin")) and (
            doc[i + 1].text[0].istitle()):
                doc[i + 1].is_sent_start = True
            elif (token.text in ["h.", "d."]) and (doc[i + 1].text[0].istitle()):  # ßand (doc[i-1].like_num):
                doc[i + 1].is_sent_start = True
            elif token.text.endswith("kappaB.") and (doc[i + 1].text[0].istitle()):
                doc[i + 1].is_sent_start = True
            elif ((token.text == "CyA.") or (token.text == "CsA.") or (token.text == "IgM.") or (
                    token.text == "IgE.")) and (doc[i + 1].text[0].istitle()):
                doc[i + 1].is_sent_start = True

        return doc

    @Language.component(name='CorpusER')
    def CorpusER(doc):
        """Main logics of the corpus-based annotation module."""

        # If the doc has some marked gold entities/triggers, annotate them
        if doc._.entities:
            annotation_gold_entity(doc,keep_ent_tokens)

        # Workaround to manage missing tensors update
        # Waiting for the fix: @[https://github.com/explosion/spaCy/issues/1963]
        doc.tensor = numpy.zeros((0,), dtype='float32')

        return doc

    def load_language_model(keep_ent_tokens):
        nlp = spacy.load("en_core_sci_lg")

        # Build a pipeline of NLP components
        nlp.remove_pipe('ner')
        nlp.add_pipe(factory_name="merger", before="tagger")
        nlp.add_pipe(factory_name="custom_sentencizer", before="parser")
        set_token_extension()
        nlp.add_pipe(factory_name="CorpusER", before='parser')
        set_document_extension()
        constrain_tokenizer(nlp)
        return nlp

    return load_language_model(keep_ent_tokens)
